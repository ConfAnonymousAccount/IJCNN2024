{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model long COVID Intensity using Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "from package.data.utils import *\n",
    "\n",
    "from package.data.lifeline_dataset import LifeLineDataSet\n",
    "from package.data.data_manager import DataManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path().absolute().parent.parent.parent\n",
    "dataset = LifeLineDataSet(data_path=data_path / \"data\" / \"extract\" / \"merged\", \n",
    "                          dataset_name=\"merged_vaccin_only_1_full.csv\",\n",
    "                          target_variable=\"long_covid_intensity\")\n",
    "dataset.get_encoded_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = DataManager()\n",
    "data_manager.split_data_train_val_test(dataset, train_size=0.7, val_test_prop=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_manager.train_dataset.features.shape)\n",
    "print(data_manager.val_dataset.features.shape)\n",
    "print(data_manager.test_dataset.features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.model.stat_models import StatisticalModels\n",
    "from package.model.statistical_models.random_forest_regressor import StatRandomForestRegressor\n",
    "from package.data.scaler import StandardScaler\n",
    "from package.data.utils import round_off_rating\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = pathlib.Path().resolve().parent / \"configurations\" / \"models\" / \"random_forest.ini\"\n",
    "\n",
    "rf_model = StatisticalModels(StatRandomForestRegressor,\n",
    "                             config_path=CONFIG_PATH,\n",
    "                             config_name=\"DEFAULT\",\n",
    "                             name=\"random_forest\",\n",
    "                             scaler=StandardScaler\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.train(data_manager.train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_model.predict(data_manager.test_dataset)\n",
    "y_predict_rounded = np.array([round_off_rating(el) for el in predictions])\n",
    "test_labels_rounded = np.array([round_off_rating(el) for el in data_manager.test_dataset.targets.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance using KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate metrics\n",
    "test_labels = data_manager.test_dataset.targets.ravel()\n",
    "print(\"MAE\")\n",
    "print(mean_absolute_error(test_labels, predictions))\n",
    "print(mean_absolute_error(test_labels_rounded, y_predict_rounded))\n",
    "print(\"MSE\")\n",
    "print(mean_squared_error(test_labels, predictions))\n",
    "print(mean_squared_error(test_labels_rounded, y_predict_rounded))\n",
    "print(\"MAPE\")\n",
    "print(mean_absolute_percentage_error(test_labels, predictions))\n",
    "print(mean_absolute_percentage_error(test_labels_rounded, y_predict_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.model.random_forest import plot_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(rf_model._model.model.feature_importances_, dataset.feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the decision tree\n",
    "rf_model_small = StatisticalModels(StatRandomForestRegressor,\n",
    "                             config_path=CONFIG_PATH,\n",
    "                             config_name=\"DEFAULT\",\n",
    "                             name=\"random_forest\",\n",
    "                             n_estimator=10,\n",
    "                             max_depth=3\n",
    "                             #scaler=StandardScaler\n",
    "                            )\n",
    "print(rf_model_small.params)\n",
    "rf_model_small.train(data_manager.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rf_model_small._model.model.estimators_))\n",
    "estimator = rf_model_small._model.model.estimators_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "_ = tree.plot_tree(estimator, feature_names=dataset.feature_list, filled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import dtreeviz\n",
    "viz = dtreeviz(estimator, dataset.features, dataset.targets,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_list)\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = pathlib.Path().resolve().parent / \"configurations\" / \"models\" / \"random_forest.ini\"\n",
    "\n",
    "rf_model = StatisticalModels(StatRandomForestRegressor,\n",
    "                             config_path=CONFIG_PATH,\n",
    "                             config_name=\"DEFAULT\",\n",
    "                             name=\"random_forest\",\n",
    "                             #scaler=StandardScaler\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics, models, predictions, true_labels = rf_model.cross_validate(dataset, n_splits=5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Statistical test\n",
    "from package.model.utils import compute_metrics\n",
    "maes = []\n",
    "pearson_tests = []\n",
    "test_value = []\n",
    "p_values = []\n",
    "for i in range(5):\n",
    "    metrics = compute_metrics(true_labels[i], predictions[i], index=-1)\n",
    "    maes.append(metrics[\"mape\"])\n",
    "    pearson_tests.append(metrics[\"pearson\"])\n",
    "    test_value.append(pearson_tests[i].statistic)\n",
    "    p_values.append(pearson_tests[i].pvalue)\n",
    "print(np.mean(test_value))\n",
    "print(np.mean(p_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lifeline",
   "language": "python",
   "name": "lifeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
